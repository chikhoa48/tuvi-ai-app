import streamlit as st
import google.generativeai as genai
from PyPDF2 import PdfReader
from pdf2image import convert_from_bytes, pdfinfo_from_bytes
import time

# --- IMPORT LIBS ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage

# --- CONFIG ---
st.set_page_config(page_title="ƒê·∫°i S∆∞ T·ª≠ Vi - V6 (Anti-Crash)", page_icon="üõ°Ô∏è", layout="wide")
st.markdown("""<style>.main {background-color: #f4f6f9;}</style>""", unsafe_allow_html=True)

# --- 1. MODEL UTILS ---
def get_available_gemini_models(api_key):
    if not api_key: return ["Nh·∫≠p Key tr∆∞·ªõc"]
    try:
        genai.configure(api_key=api_key)
        return [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods and "gemini" in m.name]
    except: return ["gemini-1.5-flash"]

# --- 2. X·ª¨ L√ù S√ÅCH AN TO√ÄN (ANTI-CRASH) ---
def ocr_batch_safe(pdf_bytes, api_key, start_page, end_page):
    """Ch·ªâ convert v√† ƒë·ªçc m·ªôt nh√≥m nh·ªè trang ƒë·ªÉ kh√¥ng n·ªï RAM"""
    try:
        # Ch·ªâ chuy·ªÉn ƒë·ªïi ƒë√∫ng s·ªë trang c·∫ßn thi·∫øt (V√≠ d·ª•: t·ª´ trang 1 ƒë·∫øn 10)
        images = convert_from_bytes(pdf_bytes, first_page=start_page, last_page=end_page)
        
        if not images: return ""

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=api_key, temperature=0)
        
        content_message = [
            {"type": "text", "text": "Ch√©p l·∫°i ch√≠nh x√°c n·ªôi dung vƒÉn b·∫£n trong c√°c trang s√°ch n√†y. Ch·ªâ l·∫•y n·ªôi dung ch·ªØ."}
        ]
        for img in images:
            content_message.append({"type": "image_url", "image_url": img})
            
        res = llm.invoke([HumanMessage(content=content_message)])
        
        # X√≥a ·∫£nh kh·ªèi b·ªô nh·ªõ ngay l·∫≠p t·ª©c
        del images
        return res.content
    except Exception as e:
        return ""

def process_pdfs_smart(pdf_docs, api_key):
    all_text = ""
    status_box = st.status("ƒêang ph√¢n t√≠ch...", expanded=True)
    
    for pdf in pdf_docs:
        file_name = pdf.name
        status_box.write(f"üìÇ ƒêang ki·ªÉm tra: {file_name}")
        
        # ƒê·ªçc file v√†o b·ªô nh·ªõ ƒë·ªám
        pdf_bytes = pdf.read()
        
        # 1. Th·ª≠ ƒë·ªçc Text tr∆∞·ªõc (Nhanh)
        try:
            pdf_reader = PdfReader(pdf)
            raw_text = ""
            for page in pdf_reader.pages:
                t = page.extract_text()
                if t: raw_text += t
        except:
            raw_text = ""

        # 2. N·∫øu l√† S√°ch Scan -> D√πng ch·∫ø ƒë·ªô 'Cu·ªën Chi·∫øu' (Safe Mode)
        if len(raw_text) < 100:
            status_box.write(f"üì∏ {file_name} l√† S√°ch Scan. ƒêang k√≠ch ho·∫°t ch·∫ø ƒë·ªô Ti·∫øt Ki·ªám RAM...")
            
            try:
                # L·∫•y t·ªïng s·ªë trang m√† kh√¥ng c·∫ßn convert ·∫£nh (Nh·∫π)
                info = pdfinfo_from_bytes(pdf_bytes)
                total_pages = info["Pages"]
                
                # Chia nh·ªè: M·ªói l·∫ßn ch·ªâ l√†m 10 trang
                CHUNK_SIZE = 10 
                ocr_full_text = ""
                
                prog_bar = status_box.progress(0, text=f"ƒêang ƒë·ªçc {file_name}...")
                
                for start in range(1, total_pages + 1, CHUNK_SIZE):
                    end = min(start + CHUNK_SIZE - 1, total_pages)
                    
                    # G·ªçi h√†m ƒë·ªçc t·ª´ng ph·∫ßn nh·ªè
                    chunk_text = ocr_batch_safe(pdf_bytes, api_key, start, end)
                    ocr_full_text += chunk_text + "\n"
                    
                    # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                    prog_bar.progress(end/total_pages, text=f"ƒê√£ ƒë·ªçc xong trang {end}/{total_pages}...")
                    time.sleep(1) # Ngh·ªâ 1 x√≠u ƒë·ªÉ gi·∫£i ph√≥ng RAM
                
                all_text += ocr_full_text
                status_box.write(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong {total_pages} trang scan.")
                
            except Exception as e:
                status_box.write(f"‚ö†Ô∏è L·ªói ƒë·ªçc file {file_name}: {e}. H√£y ki·ªÉm tra file packages.txt")
        else:
            status_box.write(f"üìù {file_name} l√† vƒÉn b·∫£n th∆∞·ªùng. ƒê√£ ƒë·ªçc xong.")
            all_text += raw_text

    status_box.update(label="Ho√†n t·∫•t!", state="complete", expanded=False)
    
    if not all_text: return None
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    return text_splitter.split_text(all_text)

# --- 3. CORE LOGIC ---
@st.cache_resource
def get_vector_store(_text_chunks, api_key):
    if not _text_chunks: return None
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    return FAISS.from_texts(texts=_text_chunks, embedding=embeddings)

def get_master_response(query, chart_data, vector_store, model_name, api_key):
    llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)
    template = "B·∫°n l√† ƒê·∫°i S∆∞ T·ª≠ Vi. D·ª±a v√†o Context v√† L√° s·ªë ƒë·ªÉ lu·∫≠n gi·∫£i.\nContext: {context}\nL√° s·ªë: {question}\nY√™u c·∫ßu: Lu·∫≠n gi·∫£i chi ti·∫øt."
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        chain_type_kwargs={"prompt": PromptTemplate.from_template(template)}
    )
    return qa_chain.invoke({"query": f"L√Å S·ªê:\n{chart_data}\n\nC√ÇU H·ªéI:\n{query}"})["result"]

def extract_chart_data(image, model_name, api_key):
    llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)
    return llm.invoke([HumanMessage(content=[{"type":"text","text":"Tr√≠ch xu·∫•t th√¥ng tin l√° s·ªë t·ª≠ vi th√†nh vƒÉn b·∫£n."},{"type":"image_url","image_url":image}])]).content

# --- MAIN ---
def main():
    st.title("üõ°Ô∏è ƒê·∫†I S∆Ø T·ª¨ VI - V6 (ANTI-CRASH)")
    
    with st.sidebar:
        api_key = st.text_input("Google API Key", type="password")
        if not api_key:
            st.info("üëà Nh·∫≠p Key ƒë·ªÉ b·∫Øt ƒë·∫ßu")
            st.stop()
            
        pdf_docs = st.file_uploader("Upload S√°ch", accept_multiple_files=True)
        if st.button("Luy·ªán H√≥a"):
            if not pdf_docs: st.warning("Ch∆∞a ch·ªçn s√°ch!"); st.stop()
            
            with st.spinner("ƒêang kh·ªüi ƒë·ªông..."):
                try:
                    chunks = process_pdfs_smart(pdf_docs, api_key)
                    if chunks:
                        st.session_state.vector_store = get_vector_store(chunks, api_key)
                        st.success("Th√†nh c√¥ng!")
                    else:
                        st.error("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung.")
                except Exception as e:
                    st.error(f"L·ªói h·ªá th·ªëng: {e}. Vui l√≤ng th·ª≠ file nh·ªè h∆°n ho·∫∑c ki·ªÉm tra packages.txt")

    if "chart_data" not in st.session_state: st.session_state.chart_data = None
    if "messages" not in st.session_state: st.session_state.messages = []

    img = st.file_uploader("·∫¢nh l√° s·ªë", type=['png','jpg'])
    if img and st.button("ƒê·ªçc L√° S·ªë"):
        st.session_state.chart_data = extract_chart_data(img, "gemini-1.5-flash", api_key)
        st.success("ƒê√£ xong!")

    for m in st.session_state.messages: st.chat_message(m["role"]).markdown(m["content"])
    
    if prompt := st.chat_input("H·ªèi ƒë·∫°i s∆∞..."):
        if "vector_store" not in st.session_state: st.warning("N·∫°p s√°ch tr∆∞·ªõc!"); st.stop()
        st.session_state.messages.append({"role":"user", "content":prompt})
        st.chat_message("user").markdown(prompt)
        with st.chat_message("assistant"):
            with st.spinner("ƒêang suy lu·∫≠n..."):
                res = get_master_response(prompt, st.session_state.chart_data, st.session_state.vector_store, "gemini-1.5-pro", api_key)
                st.markdown(res)
                st.session_state.messages.append({"role":"assistant", "content":res})

if __name__ == "__main__":
    main()
