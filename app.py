import streamlit as st
import google.generativeai as genai
from PyPDF2 import PdfReader
from pdf2image import convert_from_bytes
import io
import time

# --- IMPORT LIBS ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
from PIL import Image

# --- CONFIG ---
st.set_page_config(page_title="ƒê·∫°i S∆∞ T·ª≠ Vi - Si√™u T·ªëc (Batch)", page_icon="‚ö°", layout="wide")
st.markdown("""<style>.main {background-color: #f0f2f6;}</style>""", unsafe_allow_html=True)

# --- 1. MODEL UTILS ---
def get_available_gemini_models(api_key):
    if not api_key: return ["Nh·∫≠p Key tr∆∞·ªõc"]
    try:
        genai.configure(api_key=api_key)
        return [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods and "gemini" in m.name]
    except: return ["gemini-1.5-flash"]

# --- 2. X·ª¨ L√ù S√ÅCH SCAN (BATCH PROCESSING) ---
def process_images_in_batches(images, api_key, batch_size=20):
    """
    G·ª≠i 1 l√∫c nhi·ªÅu ·∫£nh (batch_size) cho Gemini ƒë·ªçc ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
    """
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=api_key, temperature=0)
    full_text = ""
    
    # Chia danh s√°ch ·∫£nh th√†nh c√°c g√≥i nh·ªè (chunks)
    total_images = len(images)
    
    # T·∫°o thanh ti·∫øn tr√¨nh
    progress_bar = st.progress(0, text="ƒêang kh·ªüi ƒë·ªông b·ªô m√°y ƒë·ªçc th·∫ßn t·ªëc...")
    
    for i in range(0, total_images, batch_size):
        # L·∫•y ra 1 l√¥ ·∫£nh (v√≠ d·ª• t·ª´ ·∫£nh 0 ƒë·∫øn 19)
        batch = images[i : i + batch_size]
        current_batch_num = (i // batch_size) + 1
        total_batches = (total_images + batch_size - 1) // batch_size
        
        progress_bar.progress((i / total_images), text=f"ƒêang ƒë·ªçc l√¥ {current_batch_num}/{total_batches} (Trang {i+1} ƒë·∫øn {min(i+batch_size, total_images)})...")
        
        # T·∫°o n·ªôi dung g·ª≠i ƒëi: [C√¢u l·ªánh text, ·∫¢nh 1, ·∫¢nh 2, ..., ·∫¢nh 20]
        content_message = [
            {"type": "text", "text": "B·∫°n l√† m·ªôt th∆∞ k√Ω ƒë√°nh m√°y chuy√™n nghi·ªáp. Nhi·ªám v·ª• c·ªßa b·∫°n l√† nh√¨n v√†o c√°c trang s√°ch ƒë√≠nh k√®m d∆∞·ªõi ƒë√¢y v√† ch√©p l·∫°i CH√çNH X√ÅC to√†n b·ªô n·ªôi dung vƒÉn b·∫£n trong ƒë√≥. H√£y ch√©p li·ªÅn m·∫°ch, kh√¥ng c·∫ßn m√¥ t·∫£ ·∫£nh, ch·ªâ l·∫•y n·ªôi dung ch·ªØ."}
        ]
        
        # Th√™m t·ª´ng ·∫£nh v√†o message
        for img in batch:
            content_message.append({"type": "image_url", "image_url": img})
            
        # G·ª≠i ƒëi 1 l·∫ßn duy nh·∫•t cho c·∫£ l√¥
        try:
            msg = HumanMessage(content=content_message)
            res = llm.invoke([msg])
            full_text += res.content + "\n\n"
        except Exception as e:
            st.error(f"L·ªói khi ƒë·ªçc l√¥ {current_batch_num}: {e}")
            # N·∫øu l·ªói, th·ª≠ ch·ªù 2s r·ªìi ti·∫øp t·ª•c l√¥ sau
            time.sleep(2)
            
    progress_bar.progress(1.0, text="ƒê√£ ƒë·ªçc xong to√†n b·ªô s√°ch!")
    time.sleep(1)
    progress_bar.empty()
    return full_text

def process_pdfs_smart(pdf_docs, api_key):
    all_text = ""
    status_box = st.status("ƒêang ph√¢n t√≠ch t√†i li·ªáu...", expanded=True)
    
    for pdf in pdf_docs:
        status_box.write(f"üìÇ ƒêang ki·ªÉm tra file: {pdf.name}")
        
        # 1. Th·ª≠ ƒë·ªçc Text tr∆∞·ªõc (Nhanh nh·∫•t)
        try:
            pdf_reader = PdfReader(pdf)
            raw_text = ""
            for page in pdf_reader.pages:
                t = page.extract_text()
                if t: raw_text += t
        except:
            raw_text = ""

        # 2. N·∫øu √≠t ch·ªØ qu√° -> Chuy·ªÉn sang ch·∫ø ƒë·ªô Batch OCR (Scan)
        if len(raw_text) < 100:
            status_box.write(f"üì∏ File {pdf.name} l√† d·∫°ng SCAN. ƒêang chuy·ªÉn ƒë·ªïi sang ·∫£nh...")
            pdf.seek(0)
            # Chuy·ªÉn PDF th√†nh list c√°c ·∫£nh
            images = convert_from_bytes(pdf.read())
            status_box.write(f"‚úÖ ƒê√£ t√°ch th√†nh {len(images)} trang ·∫£nh. B·∫Øt ƒë·∫ßu ƒë·ªçc Batch...")
            
            # G·ªçi h√†m x·ª≠ l√Ω h√†ng lo·∫°t
            ocr_text = process_images_in_batches(images, api_key, batch_size=20)
            all_text += ocr_text
        else:
            status_box.write(f"üìù File {pdf.name} l√† d·∫°ng vƒÉn b·∫£n. ƒê√£ ƒë·ªçc xong.")
            all_text += raw_text

    status_box.update(label="Ho√†n t·∫•t!", state="complete", expanded=False)
    
    if not all_text: return None
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    return text_splitter.split_text(all_text)

# --- 3. VECTOR STORE & RAG ---
@st.cache_resource
def get_vector_store(_text_chunks, api_key):
    if not _text_chunks: return None
    # D√πng embedding-001 cho ·ªïn ƒë·ªãnh
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    return FAISS.from_texts(texts=_text_chunks, embedding=embeddings)

def get_master_response(query, chart_data, vector_store, model_name, api_key):
    llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)
    
    template = """
    B·∫°n l√† ƒê·∫°i S∆∞ T·ª≠ Vi. D·ª±a v√†o ki·∫øn th·ª©c s√°ch (Context) v√† L√° s·ªë ƒë·ªÉ lu·∫≠n gi·∫£i.
    Context: {context}
    L√° s·ªë: {question}
    Y√™u c·∫ßu: Lu·∫≠n gi·∫£i s√¢u s·∫Øc, c√≥ d·∫´n ch·ª©ng t·ª´ s√°ch.
    """
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        chain_type_kwargs={"prompt": PromptTemplate.from_template(template)}
    )
    full_input = f"L√Å S·ªê:\n{chart_data}\n\nC√ÇU H·ªéI:\n{query}"
    return qa_chain.invoke({"query": full_input})["result"]

# --- 4. VISION (ƒê·ªåC L√Å S·ªê) ---
def extract_chart_data(image, model_name, api_key):
    llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)
    msg = HumanMessage(content=[
        {"type": "text", "text": "Tr√≠ch xu·∫•t th√¥ng tin l√° s·ªë t·ª≠ vi: Ng√†y gi·ªù, M·ªánh, Th√¢n, C·ª•c, C√°c sao t·∫°i 12 cung. Tr·∫£ v·ªÅ text."},
        {"type": "image_url", "image_url": image}
    ])
    return llm.invoke([msg]).content

# --- MAIN APP ---
def main():
    st.title("‚ö° ƒê·∫†I S∆Ø T·ª¨ VI - BATCH OCR")
    
    with st.sidebar:
        api_key = st.text_input("Google API Key", type="password")
        if not api_key: st.stop()
        
        # N·∫°p S√°ch
        pdf_docs = st.file_uploader("Upload S√°ch (PDF/Scan)", accept_multiple_files=True)
        if st.button("Luy·ªán H√≥a (Batch Mode)"):
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                chunks = process_pdfs_smart(pdf_docs, api_key)
                if chunks:
                    st.session_state.vector_store = get_vector_store(chunks, api_key)
                    st.success(f"ƒê√£ n·∫°p {len(chunks)} ƒëo·∫°n ki·∫øn th·ª©c!")
                else:
                    st.error("Kh√¥ng c√≥ n·ªôi dung!")

    # Main Interface
    if "chart_data" not in st.session_state: st.session_state.chart_data = None
    if "messages" not in st.session_state: st.session_state.messages = []

    # Upload L√° S·ªë
    img = st.file_uploader("·∫¢nh l√° s·ªë", type=['png','jpg'])
    if img and st.button("ƒê·ªçc L√° S·ªë"):
        with st.spinner("ƒêang ƒë·ªçc..."):
            # L·∫•y model t·ªët nh·∫•t t·ª´ list
            models = get_available_gemini_models(api_key)
            best_model = models[0] if models else "gemini-1.5-flash"
            st.session_state.chart_data = extract_chart_data(img, best_model, api_key)
            st.success("ƒê√£ xong!")

    # Chat
    for m in st.session_state.messages:
        st.chat_message(m["role"]).markdown(m["content"])
        
    if prompt := st.chat_input("H·ªèi ƒë·∫°i s∆∞..."):
        if "vector_store" not in st.session_state: st.warning("N·∫°p s√°ch tr∆∞·ªõc!"); return
        
        st.session_state.messages.append({"role":"user", "content":prompt})
        st.chat_message("user").markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("ƒêang suy ng·∫´m..."):
                # D√πng Gemini Pro cho c√¢u tr·∫£ l·ªùi th√¥ng minh
                models = get_available_gemini_models(api_key)
                # T√¨m model n√†o c√≥ ch·ªØ Pro, n·∫øu kh√¥ng th√¨ d√πng c√°i ƒë·∫ßu ti√™n
                chat_model = next((m for m in models if "pro" in m), models[0])
                
                res = get_master_response(prompt, st.session_state.chart_data, st.session_state.vector_store, chat_model, api_key)
                st.markdown(res)
                st.session_state.messages.append({"role":"assistant", "content":res})

if __name__ == "__main__":
    main()
