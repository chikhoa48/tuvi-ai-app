import streamlit as st
import os
import time
import google.generativeai as genai
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from PIL import Image

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(page_title="Äáº¡i SÆ° Tá»­ Vi - AI Tá»•ng Há»£p", page_icon="â›©ï¸", layout="wide")

st.markdown("""
<style>
    .main {background-color: #fdfbf7;}
    h1, h2, h3 {font-family: 'Times New Roman', serif; color: #5a1e1e;}
    .stChatInput {position: fixed; bottom: 20px;}
    .report-card {
        padding: 20px; border-radius: 10px; background-color: white;
        border-left: 5px solid #8B0000; box-shadow: 2px 2px 10px rgba(0,0,0,0.1);
        margin-bottom: 20px; font-family: 'Times New Roman', serif; font-size: 1.1em;
    }
    .reasoning-box {
        font-size: 0.9em; color: #666; font-style: italic; 
        background-color: #f0f0f0; padding: 10px; border-radius: 5px; margin-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)

# --- 1. HÃ€M Tá»° Äá»˜NG Láº¤Y MODEL Má»šI NHáº¤T ---
def get_available_gemini_models(api_key):
    """QuÃ©t API Google Ä‘á»ƒ láº¥y danh sÃ¡ch model thá»±c táº¿ Ä‘ang kháº£ dá»¥ng"""
    if not api_key:
        return ["ChÆ°a nháº­p API Key"]
    
    try:
        genai.configure(api_key=api_key)
        models = []
        for m in genai.list_models():
            # Lá»c láº¥y cÃ¡c model cÃ³ kháº£ nÄƒng táº¡o ná»™i dung (generateContent)
            if 'generateContent' in m.supported_generation_methods:
                # Æ¯u tiÃªn cÃ¡c model Gemini
                if "gemini" in m.name:
                    models.append(m.name.replace("models/", ""))
        
        # Sáº¯p xáº¿p Ä‘á»ƒ model pro/má»›i nháº¥t lÃªn Ä‘áº§u (tÃ¹y logic)
        models.sort(reverse=True)
        return models
    except Exception as e:
        return [f"Lá»—i: {str(e)}"]

# --- 2. Xá»¬ LÃ SÃCH (RAG) ---
@st.cache_resource
def get_vector_store(_text_chunks, api_key):
    # DÃ¹ng hÃ m cache Ä‘á»ƒ khÃ´ng pháº£i load láº¡i khi Ä‘á»•i model
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)
    vectorstore = FAISS.from_texts(texts=_text_chunks, embedding=embeddings)
    return vectorstore

def process_pdfs(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            t = page.extract_text()
            if t: text += t
    
    # Chia nhá» vÄƒn báº£n Ä‘á»ƒ tra cá»©u chi tiáº¿t
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
    return text_splitter.split_text(text)

# --- 3. VISION & DATA EXTRACTION ---
def extract_chart_data(image, model_name, api_key):
    llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key, temperature=0)
    msg = HumanMessage(content=[
        {"type": "text", "text": "Báº¡n lÃ  chuyÃªn gia sá»‘ hÃ³a. HÃ£y nhÃ¬n áº£nh lÃ¡ sá»‘ tá»­ vi nÃ y vÃ  trÃ­ch xuáº¥t láº¡i TOÃ€N Bá»˜ thÃ´ng tin: NgÃ y giá» sinh, Ã‚m dÆ°Æ¡ng nam/ná»¯, Cá»¥c, Má»‡nh, ThÃ¢n, vá»‹ trÃ­ 12 cung vÃ  cÃ¡c sao trong tá»«ng cung. Tráº£ vá» dáº¡ng vÄƒn báº£n cÃ³ cáº¥u trÃºc rÃµ rÃ ng."},
        {"type": "image_url", "image_url": image}
    ])
    res = llm.invoke([msg])
    return res.content

# --- 4. LOGIC Äáº I SÆ¯ (REASONING CHAIN) ---
def get_master_response(query, chart_data, vector_store, model_name, api_key, history):
    
    llm = ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key, temperature=0.5)

    # Prompt Quy náº¡p & Tá»•ng há»£p kiáº¿n thá»©c
    system_prompt = """
    Báº¡n lÃ  "Báº¡ch VÃ¢n CÆ° SÄ©" - má»™t báº­c tháº§y Tá»­ Vi Äáº©u Sá»‘, ngÆ°á»i káº¿t há»£p tinh hoa cá»§a nhiá»u trÆ°á»ng phÃ¡i.
    
    NHIá»†M Vá»¤ Cá»¦A Báº N:
    Luáº­n giáº£i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn:
    1. ThÃ´ng tin lÃ¡ sá»‘ (Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i).
    2. Kiáº¿n thá»©c tá»« cÃ¡c sÃ¡ch tá»­ vi (Ä‘Æ°á»£c cung cáº¥p trong pháº§n Context).
    
    QUY TRÃŒNH SUY LUáº¬N (Báº®T BUá»˜C):
    BÆ°á»›c 1 - Äá»‘i chiáº¿u: TÃ¬m kiáº¿m xem cÃ¡c cuá»‘n sÃ¡ch khÃ¡c nhau nÃ³i gÃ¬ vá» váº¥n Ä‘á» nÃ y (VÃ­ dá»¥: SÃ¡ch A nÃ³i sao nÃ y tá»‘t, nhÆ°ng sÃ¡ch B nÃ³i xáº¥u khi gáº·p sao kia).
    BÆ°á»›c 2 - PhÃ¢n tÃ­ch Cá»¥c diá»‡n: Xem xÃ©t ngÅ© hÃ nh, Ã¢m dÆ°Æ¡ng, vá»‹ trÃ­ Ä‘áº¯c hÃ£m Ä‘á»ƒ xem Ã½ kiáº¿n nÃ o trong sÃ¡ch lÃ  phÃ¹ há»£p nháº¥t vá»›i lÃ¡ sá»‘ nÃ y.
    BÆ°á»›c 3 - Tá»•ng há»£p (Quy náº¡p): Äá»«ng chá»‰ trÃ­ch dáº«n. HÃ£y káº¿t há»£p cÃ¡c Ã½ kiáº¿n Ä‘á»ƒ Ä‘Æ°a ra lá»i luáº­n Ä‘oÃ¡n cuá»‘i cÃ¹ng cá»§a riÃªng báº¡n.
    
    PHONG CÃCH:
    - Lá»i vÄƒn thÃ¢m tráº§m, sÃ¢u sáº¯c, cÃ³ tÃ­nh triáº¿t lÃ½.
    - LuÃ´n giáº£i thÃ­ch lÃ½ do: "SÃ¡ch Tá»­ Vi HÃ m Sá»‘ cho ráº±ng..., tuy nhiÃªn trong trÆ°á»ng há»£p nÃ y Má»‡nh báº¡n cÃ³ Tuáº§n KhÃ´ng nÃªn..."
    - TrÃ¡nh mÃ¡y mÃ³c. Náº¿u sÃ¡ch khÃ´ng cÃ³ thÃ´ng tin, hÃ£y dÃ¹ng kiáº¿n thá»©c ná»n táº£ng cá»§a báº¡n Ä‘á»ƒ suy luáº­n.

    ThÃ´ng tin lÃ¡ sá»‘ cá»§a Ä‘Æ°Æ¡ng sá»‘:
    {chart_data}

    Kiáº¿n thá»©c tham kháº£o tá»« sÃ¡ch (Context):
    {context}
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

    chain = create_stuff_documents_chain(llm, prompt)
    retriever = vector_store.as_retriever(search_kwargs={"k": 7}) # Láº¥y nhiá»u Ä‘oáº¡n vÄƒn báº£n hÆ¡n Ä‘á»ƒ tá»•ng há»£p
    rag_chain = create_retrieval_chain(retriever, chain)

    response = rag_chain.invoke({
        "input": query,
        "chart_data": chart_data,
        "chat_history": history
    })
    
    return response["answer"]

# --- GIAO DIá»†N CHÃNH ---
def main():
    st.title("â›©ï¸ THIÃŠN CÆ  CÃC - V3")
    st.caption("PhiÃªn báº£n Äáº¡i SÆ° AI: Tá»± Ä‘á»™ng cáº­p nháº­t Model & TÆ° duy quy náº¡p Ä‘a nguá»“n sÃ¡ch")

    # --- SIDEBAR ---
    with st.sidebar:
        st.header("ğŸ”‘ ChÃ¬a khÃ³a & TÃ ng thÆ°")
        api_key = st.text_input("Nháº­p Google AI Key", type="password")
        
        # --- AUTO UPDATE MODEL SELECTOR ---
        if api_key:
            st.success("ÄÃ£ káº¿t ná»‘i Google AI!")
            available_models = get_available_gemini_models(api_key)
            selected_model = st.selectbox("Chá»n 'Linh Há»“n' (Model) cho Äáº¡i SÆ°:", available_models, index=0)
            if "gemini-1.5-pro" in selected_model or "gemini-2" in selected_model:
                st.info("ğŸ’¡ Model nÃ y cÃ³ kháº£ nÄƒng suy luáº­n máº¡nh máº½ nháº¥t.")
        else:
            selected_model = "gemini-1.5-pro" # Default áº£o
            st.warning("Vui lÃ²ng nháº­p API Key Ä‘á»ƒ táº£i danh sÃ¡ch Model má»›i nháº¥t.")

        st.divider()
        st.subheader("ğŸ“š Náº¡p Kiáº¿n Thá»©c (SÃ¡ch)")
        pdf_docs = st.file_uploader("Upload sÃ¡ch (.pdf)", accept_multiple_files=True)
        
        if st.button("Luyá»‡n HÃ³a Kiáº¿n Thá»©c"):
            if not pdf_docs or not api_key:
                st.error("Thiáº¿u nguyÃªn liá»‡u!")
            else:
                with st.spinner("Äang Ä‘á»c vÃ  Ä‘á»‘i chiáº¿u cÃ¡c sÃ¡ch..."):
                    chunks = process_pdfs(pdf_docs)
                    st.session_state.vector_store = get_vector_store(chunks, api_key)
                    st.success(f"ÄÃ£ háº¥p thá»¥ {len(chunks)} Ä‘Æ¡n vá»‹ kiáº¿n thá»©c!")

    # --- MAIN AREA ---
    
    # 1. Upload & PhÃ¢n tÃ­ch áº£nh (Chá»‰ lÃ m 1 láº§n)
    if "chart_data" not in st.session_state:
        st.session_state.chart_data = None

    uploaded_img = st.file_uploader("BÆ°á»›c 1: Táº£i áº£nh lÃ¡ sá»‘ lÃªn Ä‘á»ƒ Äáº¡i sÆ° xem qua", type=['png', 'jpg', 'jpeg'])
    
    if uploaded_img and not st.session_state.chart_data:
        if st.button("TrÃ­ch xuáº¥t thÃ´ng tin lÃ¡ sá»‘"):
            if not api_key: st.error("Cáº§n API Key."); return
            with st.spinner("Äang quan sÃ¡t tinh bÃ n..."):
                st.image(uploaded_img, width=300)
                # DÃ¹ng model vision Ä‘á»c áº£nh
                data = extract_chart_data(uploaded_img, selected_model, api_key)
                st.session_state.chart_data = data
                st.success("ÄÃ£ náº¯m rÃµ cÃ¡ch cá»¥c lÃ¡ sá»‘!")
                with st.expander("Xem thÃ´ng tin thÃ´ (Debug)"):
                    st.write(data)

    # 2. Khu vá»±c TrÃ² chuyá»‡n / Luáº­n Ä‘oÃ¡n
    if st.session_state.chart_data:
        st.divider()
        st.subheader("ğŸ”® Äá»‘i thoáº¡i cÃ¹ng Äáº¡i SÆ°")
        
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
        for msg in st.session_state.messages:
            role = "user" if isinstance(msg, HumanMessage) else "assistant"
            with st.chat_message(role):
                st.markdown(msg.content)

        # Input ngÆ°á»i dÃ¹ng
        user_query = st.chat_input("Há»i Äáº¡i sÆ° (VD: 'Luáº­n cung TÃ i Báº¡ch cá»§a tÃ´i?', 'NÄƒm nay váº­n háº¡n ra sao?')")
        
        if user_query:
            if "vector_store" not in st.session_state:
                st.error("Äáº¡i sÆ° chÆ°a Ä‘Æ°á»£c há»c sÃ¡ch (ChÆ°a upload sÃ¡ch bÃªn trÃ¡i)!")
            else:
                # Hiá»ƒn thá»‹ cÃ¢u há»i
                st.chat_message("user").markdown(user_query)
                st.session_state.messages.append(HumanMessage(content=user_query))
                
                # AI xá»­ lÃ½
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    
                    # Hiá»ƒn thá»‹ tráº¡ng thÃ¡i "Suy nghÄ©"
                    with st.status("Äang tra cá»©u vÃ  quy náº¡p kiáº¿n thá»©c...", expanded=True) as status:
                        st.write("ğŸ” Äang tÃ¬m cÃ¡c Ä‘oáº¡n liÃªn quan trong sÃ¡ch...")
                        st.write("âš–ï¸ Äang so sÃ¡nh cÃ¡c thuyáº¿t khÃ¡c nhau...")
                        st.write("âœï¸ Äang tá»•ng há»£p lá»i luáº­n...")
                        
                        # Gá»i hÃ m xá»­ lÃ½ chÃ­nh
                        response_text = get_master_response(
                            user_query,
                            st.session_state.chart_data,
                            st.session_state.vector_store,
                            selected_model,
                            api_key,
                            st.session_state.messages
                        )
                        status.update(label="ÄÃ£ luáº­n giáº£i xong!", state="complete", expanded=False)
                    
                    message_placeholder.markdown(response_text)
                    st.session_state.messages.append(AIMessage(content=response_text))

if __name__ == "__main__":
    main()
